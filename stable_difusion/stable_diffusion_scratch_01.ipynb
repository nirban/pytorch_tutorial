{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/nirban/pytorch_tutorial/blob/main/stable_difusion/stable_diffusion_scratch_01.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Stable Diffusion fronm Scratch V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'StableDiffusion-PyTorch'...\n",
      "remote: Enumerating objects: 144, done.\u001b[K\n",
      "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
      "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
      "remote: Total 144 (delta 37), reused 17 (delta 17), pack-reused 86 (from 1)\u001b[K\n",
      "Receiving objects: 100% (144/144), 63.79 KiB | 395.00 KiB/s, done.\n",
      "Resolving deltas: 100% (74/74), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/nirban/StableDiffusion-PyTorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"StableDiffusion-PyTorch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "## Data Preparation\n",
    "### Mnist\n",
    "\n",
    "For setting up the mnist dataset follow - https://github.com/explainingai-code/Pytorch-VAE#data-preparation\n",
    "\n",
    "Ensure directory structure is following\n",
    "```\n",
    "StableDiffusion-PyTorch\n",
    "    -> data\n",
    "        -> mnist\n",
    "            -> train\n",
    "                -> images\n",
    "                    -> *.png\n",
    "            -> test\n",
    "                -> images\n",
    "                    -> *.png\n",
    "```\n",
    "\n",
    "### CelebHQ \n",
    "#### Unconditional\n",
    "For setting up on CelebHQ for unconditional, simply download the images from the official repo of CelebMASK HQ [here](https://github.com/switchablenorms/CelebAMask-HQ?tab=readme-ov-file).\n",
    "\n",
    "Ensure directory structure is the following\n",
    "```\n",
    "StableDiffusion-PyTorch\n",
    "    -> data\n",
    "        -> CelebAMask-HQ\n",
    "            -> CelebA-HQ-img\n",
    "                -> *.jpg\n",
    "\n",
    "```\n",
    "#### Mask Conditional\n",
    "For CelebHQ for mask conditional LDM additionally do the following:\n",
    "\n",
    "Ensure directory structure is the following\n",
    "```\n",
    "StableDiffusion-PyTorch\n",
    "    -> data\n",
    "        -> CelebAMask-HQ\n",
    "            -> CelebA-HQ-img\n",
    "                -> *.jpg\n",
    "            -> CelebAMask-HQ-mask-anno\n",
    "                -> 0/1/2/3.../14\n",
    "                    -> *.png\n",
    "            \n",
    "```\n",
    "\n",
    "* Run `python -m utils.create_celeb_mask` from repo root to create the mask images from mask annotations\n",
    "\n",
    "Ensure directory structure is the following\n",
    "```\n",
    "StableDiffusion-PyTorch\n",
    "    -> data\n",
    "        -> CelebAMask-HQ\n",
    "            -> CelebA-HQ-img\n",
    "                -> *.jpg\n",
    "            -> CelebAMask-HQ-mask-anno\n",
    "                -> 0/1/2/3.../14\n",
    "                    -> *.png\n",
    "            -> CelebAMask-HQ-mask\n",
    "                  -> *.png\n",
    "```\n",
    "\n",
    "#### Text Conditional\n",
    "For CelebHQ for text conditional LDM additionally do the following:\n",
    "* The repo uses captions collected as part of this repo - https://github.com/IIGROUP/MM-CelebA-HQ-Dataset?tab=readme-ov-file \n",
    "* Download the captions from the `text` link provided in the repo - https://github.com/IIGROUP/MM-CelebA-HQ-Dataset?tab=readme-ov-file#overview\n",
    "* This will download a `celeba-captions` folder, simply move this inside the `data/CelebAMask-HQ` folder as that is where the dataset class expects it to be.\n",
    "\n",
    "Ensure directory structure is the following\n",
    "```\n",
    "StableDiffusion-PyTorch\n",
    "    -> data\n",
    "        -> CelebAMask-HQ\n",
    "            -> CelebA-HQ-img\n",
    "                -> *.jpg\n",
    "            -> CelebAMask-HQ-mask-anno\n",
    "                -> 0/1/2/3.../14\n",
    "                    -> *.png\n",
    "            -> CelebAMask-HQ-mask\n",
    "                -> *.png\n",
    "            -> celeba-caption\n",
    "                -> *.txt\n",
    "```\n",
    "---\n",
    "## Configuration\n",
    " Allows you to play with different components of ddpm and autoencoder training\n",
    "* ```config/mnist.yaml``` - Small autoencoder and ldm can even be trained on CPU\n",
    "* ```config/celebhq.yaml``` - Configuration used for celebhq dataset\n",
    "\n",
    "Relevant configuration parameters\n",
    "\n",
    "Most parameters are self explanatory but below I mention couple which are specific to this repo.\n",
    "* ```autoencoder_acc_steps``` : For accumulating gradients if image size is too large for larger batch sizes\n",
    "* ```save_latents``` : Enable this to save the latents , during inference of autoencoder. That way ddpm training will be faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
