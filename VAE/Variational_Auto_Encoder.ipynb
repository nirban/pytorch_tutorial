{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/nirban/pytorch_tutorial/blob/main/VAE/Variational_Auto_Encoder.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Variational Auto Encoder From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import random\n",
    "import shutil\n",
    "import argparse\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import _csv as csv\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "A very simple VAE which has the following architecture\n",
    "Encoder\n",
    "    For Conditional model we stack num_classes empty channels onto the image\n",
    "        We make the gt_label index channel as `1` (See figure in README)\n",
    "    N * Conv BN Activation Blocks\n",
    "    FC layers for mean\n",
    "    FC layers for variance\n",
    "\n",
    "Decoder\n",
    "    For Conditional model we also concat the one hot label feature onto the z input\n",
    "        (See figure in README)\n",
    "    FC Layers taking z to higher dimensional feature\n",
    "    N * ConvTranspose BN Activation Blocks\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 config\n",
    "                 ):\n",
    "        super(VAE, self).__init__()\n",
    "        activation_map = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky': nn.LeakyReLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'gelu': nn.GELU(),\n",
    "            'silu': nn.SiLU()\n",
    "        }\n",
    "        \n",
    "        self.config = config\n",
    "        ##### Validate the configuration for the model is correctly setup #######\n",
    "        assert config['transpose_activation_fn'] is None or config['transpose_activation_fn'] in activation_map\n",
    "        assert config['dec_fc_activation_fn'] is None or config['dec_fc_activation_fn'] in activation_map\n",
    "        assert config['conv_activation_fn'] is None or config['conv_activation_fn'] in activation_map\n",
    "        assert config['enc_fc_activation_fn'] is None or config['enc_fc_activation_fn'] in activation_map\n",
    "        assert config['enc_fc_layers'][-1] == config['dec_fc_layers'][0] == config['latent_dim'], \\\n",
    "            \"Latent dimension must be same as fc layers number\"\n",
    "        \n",
    "        self.num_classes = config['num_classes']\n",
    "        self.transposebn_channels = config['transposebn_channels']\n",
    "        self.latent_dim = config['latent_dim']\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Number of input channels will change if its a conditional model\n",
    "        if config['concat_channel'] and config['conditional']:\n",
    "            config['convbn_channels'][0] += self.num_classes\n",
    "        \n",
    "        # Encoder is just Conv bn blocks followed by fc for mean and variance\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(config['convbn_channels'][i], config['convbn_channels'][i + 1],\n",
    "                          kernel_size=config['conv_kernel_size'][i], stride=config['conv_kernel_strides'][i]),\n",
    "                nn.BatchNorm2d(config['convbn_channels'][i + 1]),\n",
    "                activation_map[config['conv_activation_fn']]\n",
    "            )\n",
    "            for i in range(config['convbn_blocks'])\n",
    "        ])\n",
    "        \n",
    "        encoder_mu_activation = nn.Identity() if config['enc_fc_mu_activation'] is None else activation_map[\n",
    "            config['enc_fc_mu_activation']]\n",
    "        self.encoder_mu_fc = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(config['enc_fc_layers'][i], config['enc_fc_layers'][i + 1]),\n",
    "                encoder_mu_activation\n",
    "            )\n",
    "            for i in range(len(config['enc_fc_layers']) - 1)\n",
    "        ])\n",
    "        encoder_var_activation = nn.Identity() if config['enc_fc_var_activation'] is None else activation_map[\n",
    "            config['enc_fc_var_activation']]\n",
    "        self.encoder_var_fc = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(config['enc_fc_layers'][i], config['enc_fc_layers'][i + 1]),\n",
    "                encoder_var_activation\n",
    "            )\n",
    "            for i in range(len(config['enc_fc_layers']) - 1)\n",
    "        ])\n",
    "        \n",
    "        # Number of features will change if it's a conditional model\n",
    "        if config['decoder_fc_condition'] and config['conditional']:\n",
    "            config['dec_fc_layers'][0] += self.num_classes\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(config['transposebn_channels'][i], config['transposebn_channels'][i + 1],\n",
    "                                   kernel_size=config['transpose_kernel_size'][i],\n",
    "                                   stride=config['transpose_kernel_strides'][i]),\n",
    "                nn.BatchNorm2d(config['transposebn_channels'][i + 1]),\n",
    "                activation_map[config['transpose_activation_fn']]\n",
    "            )\n",
    "            for i in range(config['transpose_bn_blocks'])\n",
    "        ])\n",
    "        \n",
    "        self.decoder_fc = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(config['dec_fc_layers'][i], config['dec_fc_layers'][i + 1]),\n",
    "                activation_map[config['dec_fc_activation_fn']]\n",
    "            )\n",
    "            for i in range(len(config['dec_fc_layers']) - 1)\n",
    "        \n",
    "        ])\n",
    "\n",
    "    def forward(self, x, label=None):\n",
    "        out = x\n",
    "        if self.config['concat_channel'] and self.config['conditional']:\n",
    "            # Stack the label feature maps onto the input if its a conditional model\n",
    "            # And config asks to do so\n",
    "            label_ch_map = torch.zeros((x.size(0), self.num_classes, *x.shape[2:])).to(self.device)\n",
    "            batch_idx, label_idx = (torch.arange(0, x.size(0), device=self.device),\n",
    "                                    label[torch.arange(0, x.size(0), device=self.device)])\n",
    "            label_ch_map[batch_idx, label_idx, :, :] = 1\n",
    "            out = torch.cat([x, label_ch_map], dim=1)\n",
    "            \n",
    "        for layer in self.encoder_layers:\n",
    "            out = layer(out)\n",
    "        out = out.reshape((x.size(0), -1))\n",
    "        mu = out\n",
    "        for layer in self.encoder_mu_fc:\n",
    "            mu = layer(mu)\n",
    "        std = out\n",
    "        for layer in self.encoder_var_fc:\n",
    "            std = layer(std)\n",
    "\n",
    "        z = self.reparameterize(mu, std)\n",
    "        generated_out = self.generate(z, label)\n",
    "        if self.config['log_variance']:\n",
    "            return {\n",
    "                'mean': mu,\n",
    "                'log_variance': std,\n",
    "                'image': generated_out,\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'mean': mu,\n",
    "                'std': std,\n",
    "                'image': generated_out,\n",
    "            }\n",
    "    \n",
    "    def generate(self, z, label=None):\n",
    "        out = z\n",
    "        if self.config['decoder_fc_condition'] and self.config['conditional']:\n",
    "            assert label is not None, \"Label cannot be none for conditional generation\"\n",
    "            # Concat the num_classes dimensional one hot feature vector onto z\n",
    "            # For label 9 this will be [0,0,0,0,0,0,0,0,0,1]\n",
    "            label_fc_input = torch.zeros((z.size(0), self.num_classes)).to(self.device)\n",
    "            batch_idx, label_idx = (torch.arange(0, z.size(0), device=self.device),\n",
    "                                    label[torch.arange(0, z.size(0), device=self.device)])\n",
    "            label_fc_input[batch_idx, label_idx] = 1\n",
    "            out = torch.cat([out, label_fc_input], dim=-1)\n",
    "        for layer in self.decoder_fc:\n",
    "            out = layer(out)\n",
    "        # Figure out how to reshape based on desired number of channels in transpose convolution\n",
    "        hw = torch.as_tensor(out.size(-1) / self.transposebn_channels[0]).to(self.device)\n",
    "        spatial = int(torch.sqrt(hw))\n",
    "        assert spatial * spatial == hw\n",
    "        out = out.reshape((z.size(0), -1, spatial, spatial))\n",
    "        for layer in self.decoder_layers:\n",
    "            out = layer(out)\n",
    "        return out\n",
    "    \n",
    "    def sample(self, label=None, num_images=1, z=None):\n",
    "        if z is None:\n",
    "            z = torch.randn((num_images, self.latent_dim))\n",
    "        if self.config['conditional']:\n",
    "            assert label is not None, \"Label cannot be none for conditional sampling\"\n",
    "            assert label.size(0) == num_images\n",
    "        assert z.size(0) == num_images\n",
    "        out = self.generate(z, label)\n",
    "        return out\n",
    "    \n",
    "    def reparameterize(self, mu, std_or_logvariance):\n",
    "        if self.config['log_variance']:\n",
    "            std = torch.exp(0.5 * std_or_logvariance)\n",
    "        else:\n",
    "            std = std_or_logvariance\n",
    "        z = torch.randn_like(std)\n",
    "        return z * std + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config):\n",
    "    model = VAE(\n",
    "        config=config['model_params']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir config\n",
    "! cd config\n",
    "! pwd\n",
    "! wget https://raw.githubusercontent.com/explainingai-code/VAE-Pytorch/refs/heads/main/config/vae_kl_latent4.yaml -P config/\n",
    "! wget https://raw.githubusercontent.com/explainingai-code/VAE-Pytorch/refs/heads/main/config/vae_kl.yaml -P config/\n",
    "! wget https://raw.githubusercontent.com/explainingai-code/VAE-Pytorch/refs/heads/main/config/vae_kl_latent4_enc_channel_dec_fc_condition.yaml -P config/\n",
    "! wget https://raw.githubusercontent.com/explainingai-code/VAE-Pytorch/refs/heads/main/config/vae_nokl.yaml -P config/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.append('../')\n",
    "import yaml\n",
    "\n",
    "config_path = '../config/vae_kl_latent4.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "    \n",
    "model = get_model(config)\n",
    "labels = torch.zeros((3)).long()\n",
    "labels[0] = 0\n",
    "labels[1] = 2\n",
    "out = model(torch.rand((3,1,28,28)), labels)\n",
    "print(out['mean'].shape)\n",
    "print(out['image'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data/train/images\n",
    "! mkdir -p data/test/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 15.2M  100 15.2M    0     0  3988k      0  0:00:03  0:00:03 --:--:-- 6773k\n"
     ]
    }
   ],
   "source": [
    "#!/bin/bash\n",
    "! curl -L -o archive.zip https://www.kaggle.com/api/v1/datasets/download/oddrationale/mnist-in-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(save_dir, csv_fname):\n",
    "    assert os.path.exists(save_dir), \"Directory {} to save images does not exist\".format(save_dir)\n",
    "    assert os.path.exists(csv_fname), \"Csv file {} does not exist\".format(csv_fname)\n",
    "    with open(csv_fname) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for idx, row in enumerate(reader):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            im = np.zeros((784))\n",
    "            im[:] = list(map(int, row[1:]))\n",
    "            im = im.reshape((28,28))\n",
    "            if not os.path.exists(os.path.join(save_dir, row[0])):\n",
    "                os.mkdir(os.path.join(save_dir, row[0]))\n",
    "            cv2.imwrite(os.path.join(save_dir, row[0], '{}.png'.format(idx)), im)\n",
    "            if idx % 1000 == 0:\n",
    "                print('Finished creating {} images in {}'.format(idx+1, save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_images('data/train/images', 'data/mnist_train.csv')\n",
    "extract_images('data/test/images', 'data/mnist_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataset(Dataset):\n",
    "    def __init__(self, split, im_path, im_ext='png'):\n",
    "        self.split = split\n",
    "        self.im_ext = im_ext\n",
    "        self.images, self.labels = self.load_images(im_path)\n",
    "        \n",
    "    def load_images(self, im_path):\n",
    "        assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n",
    "        ims = []\n",
    "        labels = []\n",
    "        for d_name in tqdm(os.listdir(im_path)):\n",
    "            for fname in glob.glob(os.path.join(im_path, d_name, '*.{}'.format(self.im_ext))):\n",
    "                ims.append(fname)\n",
    "                labels.append(int(d_name))\n",
    "        print('Found {} images for split {}'.format(len(ims), self.split))\n",
    "        return ims, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        im = cv2.imread(self.images[index], 0)\n",
    "        label = self.labels[index]\n",
    "        # Convert to 0 to 255 into -1 to 1\n",
    "        im = 2*(im / 255) - 1\n",
    "        # Convert H,W,C into 1,C,H,W\n",
    "        im_tensor = torch.from_numpy(im)[None,:]\n",
    "        return im_tensor, torch.as_tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MnistDataset('test', 'data/test/images')\n",
    "mnist_loader = DataLoader(mnist, batch_size=16, shuffle=True, num_workers=0)\n",
    "for im, label in mnist_loader:\n",
    "    print('Image dimension', im.shape)\n",
    "    print('Label dimension: {}'.format(label.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torchvision\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_space(config, model, data_loader, save_fig_path):\n",
    "    r\"\"\"\n",
    "    Method to visualize the latent dimension by simply plotting the means for each of the images\n",
    "    :param config: Config file used to create the model\n",
    "    :param model:\n",
    "    :param data_loader:\n",
    "    :param save_fig_path: Path where the latent space image will be saved\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    means = []\n",
    "    \n",
    "    for im, label in tqdm(data_loader):\n",
    "        im = im.float().to(device)\n",
    "        label = label.long().to(device)\n",
    "        output = model(im, label)\n",
    "        labels.append(label)\n",
    "        mean = output['mean']\n",
    "        means.append(mean)\n",
    "    \n",
    "    labels = torch.cat(labels, dim=0).reshape(-1)\n",
    "    means = torch.cat(means, dim=0)\n",
    "    if model.latent_dim != 2:\n",
    "        print('Latent dimension > 2 and hence projecting')\n",
    "        U, _, V = torch.pca_lowrank(means, center=True, niter=2)\n",
    "        proj_means = torch.matmul(means, V[:, :2])\n",
    "        if not os.path.exists(config['train_params']['task_name']):\n",
    "            os.mkdir(config['train_params']['task_name'])\n",
    "        pickle.dump(V, open('{}/pca_matrix.pkl'.format(config['train_params']['task_name']), 'wb'))\n",
    "        means = proj_means\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    for num in range(10):\n",
    "        idxs = torch.where(labels == num)[0]\n",
    "        ax.scatter(means[idxs, 0].cpu().numpy(), means[idxs, 1].cpu().numpy(), s=10, label=str(num),\n",
    "                   alpha=1.0, edgecolors='none')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.savefig(save_fig_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(config, model, dataset, num_images=100):\n",
    "    r\"\"\"\n",
    "    Randomly sample points from the dataset and visualize image and its reconstruction\n",
    "    :param config: Config file used to create the model\n",
    "    :param model: Trained model\n",
    "    :param dataset: Mnist dataset(not the data loader)\n",
    "    :param num_images: NUmber of images to visualize\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('Generating reconstructions')\n",
    "    if not os.path.exists(config['train_params']['task_name']):\n",
    "        os.mkdir(config['train_params']['task_name'])\n",
    "    if not os.path.exists(\n",
    "            os.path.join(config['train_params']['task_name'], config['train_params']['output_train_dir'])):\n",
    "        os.mkdir(os.path.join(config['train_params']['task_name'], config['train_params']['output_train_dir']))\n",
    "    \n",
    "    idxs = torch.randint(0, len(dataset) - 1, (num_images,))\n",
    "    ims = torch.cat([dataset[idx][0][None, :] for idx in idxs]).float()\n",
    "    labels = torch.cat([dataset[idx][1][None] for idx in idxs]).long()\n",
    "    \n",
    "    output = model(ims, labels)\n",
    "    generated_im = output['image']\n",
    "    \n",
    "    # Dataset generates -1 to 1 we convert it to 0-1\n",
    "    ims = (ims + 1) / 2\n",
    "    # For reconstruction, we specifically flip it(white digit on black background -> black digit on white background)\n",
    "    # for easier visualization\n",
    "    generated_im = 1 - (generated_im + 1) / 2\n",
    "    out = torch.hstack([ims, generated_im])\n",
    "    output = rearrange(out, 'b c h w -> b () h (c w)')\n",
    "    grid = make_grid(output, nrow=10)\n",
    "    img = torchvision.transforms.ToPILImage()(grid)\n",
    "    img.save(os.path.join(config['train_params']['task_name'],\n",
    "                          config['train_params']['output_train_dir'],\n",
    "                          'reconstruction.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_interpolation(config, model, dataset, interpolation_steps=500, save_dir='interp'):\n",
    "    r\"\"\"\n",
    "        We randomly fetch two points and linearly interpolate between them.\n",
    "        We only use the mean values for interpolation\n",
    "    :param config:\n",
    "    :param model:\n",
    "    :param dataset:\n",
    "    :param interpolation_steps: We will interpolate these many points between start and end\n",
    "    :param save_dir:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # if model.config['conditional']:\n",
    "    #     print('Interpolation is only for non conditional model. Check README for details. Skipping...')\n",
    "    #     return\n",
    "    print('Interpolating between images')\n",
    "    if not os.path.exists(config['train_params']['task_name']):\n",
    "        os.mkdir(config['train_params']['task_name'])\n",
    "    if not os.path.exists(\n",
    "            os.path.join(config['train_params']['task_name'], config['train_params']['output_train_dir'])):\n",
    "        os.mkdir(os.path.join(config['train_params']['task_name'], config['train_params']['output_train_dir']))\n",
    "    \n",
    "    if os.path.exists(os.path.join(config['train_params']['task_name'],\n",
    "                                   config['train_params']['output_train_dir'],\n",
    "                                   save_dir)):\n",
    "        shutil.rmtree(os.path.join(config['train_params']['task_name'],\n",
    "                                   config['train_params']['output_train_dir'],\n",
    "                                   save_dir))\n",
    "    os.mkdir(os.path.join(config['train_params']['task_name'],\n",
    "                          config['train_params']['output_train_dir'],\n",
    "                          save_dir))\n",
    "    \n",
    "    idxs = torch.randint(0, len(dataset)-1, (2,))\n",
    "    if model.config['conditional']:\n",
    "        label_val = torch.randint(0, 9, (1,))\n",
    "        labels = (torch.ones((1,)).long().to(device) * label_val).repeat((2))\n",
    "    else:\n",
    "        labels = None\n",
    "    ims = torch.cat([dataset[idx][0][None, :] for idx in idxs]).float()\n",
    "    means = model(ims, labels)['mean']\n",
    "    factors = torch.linspace(0, 1.0, steps=interpolation_steps)\n",
    "    means_start = means[0]\n",
    "    means_end = means[1]\n",
    "    if model.config['conditional']:\n",
    "        label_val = torch.randint(0, 9, (1,))\n",
    "        labels = (torch.ones((1,)).long().to(device) * label_val).repeat((interpolation_steps))\n",
    "    else:\n",
    "        labels = None\n",
    "    means = factors[:, None] * means_end[None, :] + (1 - factors[:, None]) * means_start[None, :]\n",
    "    out = model.generate(means, labels)\n",
    "    for idx in tqdm(range(out.shape[0])):\n",
    "        # Convert generated output from -1 to 1 range to 0-255\n",
    "        im = 255 * (out[idx, 0] + 1) / 2\n",
    "        cv2.imwrite('{}/{}.png'.format(os.path.join(config['train_params']['task_name'],\n",
    "                                                    config['train_params']['output_train_dir'],\n",
    "                                                    save_dir), idx), im.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_manifold(config, model):\n",
    "    print('Generating the manifold')\n",
    "    if not os.path.exists(config['train_params']['task_name']):\n",
    "        os.mkdir(config['train_params']['task_name'])\n",
    "    if not os.path.exists(\n",
    "            os.path.join(config['train_params']['task_name'], config['train_params']['output_train_dir'])):\n",
    "        os.mkdir(os.path.join(config['train_params']['task_name'], config['train_params']['output_train_dir']))\n",
    "    \n",
    "    # For conditional model we can generate all numbers for all points in the space.\n",
    "    # This because the condition introduces the variance even if the point (z) is the same\n",
    "    # But for non-conditional only one output is possible for one z hence progress bar range is 1\n",
    "    if model.config['conditional']:\n",
    "        pbar_range = model.num_classes\n",
    "    else:\n",
    "        pbar_range = 1\n",
    "    for label_val in tqdm(range(pbar_range)):\n",
    "        num_images = 900\n",
    "        # For values below use the latent images to get a sense of what ranges we need to plot\n",
    "        xs = torch.linspace(-10, 10, 30)\n",
    "        ys = torch.linspace(-10, 10, 30)\n",
    "        \n",
    "        xs, ys = torch.meshgrid([xs, ys])\n",
    "        xs = xs.reshape(-1, 1)\n",
    "        ys = ys.reshape(-1, 1)\n",
    "        zs = torch.cat([xs, ys], dim=-1)\n",
    "        if model.latent_dim != 2:\n",
    "            if not os.path.exists(os.path.join(config['train_params']['task_name'], 'pca_matrix.pkl')):\n",
    "                print('Latent dimension > 2 but no pca info available. '\n",
    "                      'Call visualize_latent_space first. Skipping visualize_manifold')\n",
    "            else:\n",
    "                V = pickle.load(open(os.path.join(config['train_params']['task_name'], 'pca_matrix.pkl'), 'rb'))\n",
    "                reconstruct_means = torch.matmul(zs, V[:, :2].T)\n",
    "                zs = reconstruct_means\n",
    "        label = (torch.ones((1,)).long().to(device) * label_val).repeat((num_images))\n",
    "        generated_ims = model.sample(label, num_images, z=zs)\n",
    "        generated_ims = ((generated_ims + 1) / 2)\n",
    "        grid = make_grid(generated_ims, nrow=30)\n",
    "        img = torchvision.transforms.ToPILImage()(grid)\n",
    "        img.save(os.path.join(config['train_params']['task_name'],\n",
    "                              config['train_params']['output_train_dir'],\n",
    "                              'manifold_{}.png'.format(label_val) if model.config['conditional'] else 'manifold.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_one_epoch(epoch_idx, model, mnist_loader, optimizer, crtierion, config):\n",
    "    r\"\"\"\n",
    "    Method to run the training for one epoch.\n",
    "    :param epoch_idx: iteration number of current epoch\n",
    "    :param model: VAE model\n",
    "    :param mnist_loader: Data loder for mnist\n",
    "    :param optimizer: optimzier to be used taken from config\n",
    "    :param crtierion: For computing the loss\n",
    "    :param config: configuration for the current run\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    recon_losses = []\n",
    "    kl_losses = []\n",
    "    losses = []\n",
    "    # We ignore the label for VAE\n",
    "    for im, label in tqdm(mnist_loader):\n",
    "        im = im.float().to(device)\n",
    "        label = label.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(im, label)\n",
    "        mean = output['mean']\n",
    "        std, log_variance = None, None\n",
    "        if config['model_params']['log_variance']:\n",
    "            log_variance = output['log_variance']\n",
    "        else:\n",
    "            std = output['std']\n",
    "        generated_im = output['image']\n",
    "        if config['train_params']['save_training_image']:\n",
    "            cv2.imwrite('input.jpeg', (255 * (im.detach() + 1) / 2).cpu().numpy()[0, 0])\n",
    "            cv2.imwrite('output.jpeg', (255 * (generated_im.detach() + 1) / 2).cpu().numpy()[0, 0])\n",
    "        \n",
    "        if config['model_params']['log_variance']:\n",
    "            kl_loss = torch.mean(0.5 * torch.sum(torch.exp(log_variance) + mean ** 2 - 1 - log_variance, dim=-1))\n",
    "        else:\n",
    "            kl_loss = torch.mean(0.5 * torch.sum(std ** 2 + mean ** 2 - 1 - torch.log(std ** 2), dim=-1))\n",
    "        recon_loss = crtierion(generated_im, im)\n",
    "        loss = recon_loss + config['train_params']['kl_weight'] * kl_loss\n",
    "        recon_losses.append(recon_loss.item())\n",
    "        losses.append(loss.item())\n",
    "        kl_losses.append(kl_loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Finished epoch: {} | Recon Loss : {:.4f} | KL Loss : {:.4f}'.format(epoch_idx + 1,\n",
    "                                                                               np.mean(recon_losses),\n",
    "                                                                               np.mean(kl_losses)))\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    ######## Read the config file #######\n",
    "    with open(args.config_path, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    print(config)\n",
    "    #######################################\n",
    "    \n",
    "    ######## Set the desired seed value #######\n",
    "    seed = config['train_params']['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "    #######################################\n",
    "    \n",
    "    # Create the model and dataset\n",
    "    model = get_model(config).to(device)\n",
    "    mnist = MnistDataset('train', config['train_params']['train_path'])\n",
    "    mnist_test = MnistDataset('test', config['train_params']['test_path'])\n",
    "    mnist_loader = DataLoader(mnist, batch_size=config['train_params']['batch_size'], shuffle=True, num_workers=4)\n",
    "    mnist_test_loader = DataLoader(mnist_test, batch_size=config['train_params']['batch_size'], shuffle=False,\n",
    "                                   num_workers=0)\n",
    "    num_epochs = config['train_params']['epochs']\n",
    "    optimizer = Adam(model.parameters(), lr=config['train_params']['lr'])\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=1, verbose=True)\n",
    "    criterion = {\n",
    "        'l1': torch.nn.L1Loss(),\n",
    "        'l2': torch.nn.MSELoss()\n",
    "    }.get(config['train_params']['crit'])\n",
    "    \n",
    "    # Deleting old outputs for this task\n",
    "    # Create output directories\n",
    "    if os.path.exists(config['train_params']['task_name']):\n",
    "        shutil.rmtree(config['train_params']['task_name'])\n",
    "    os.mkdir(config['train_params']['task_name'])\n",
    "    os.mkdir(os.path.join(config['train_params']['task_name'], config['train_params']['output_train_dir']))\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    latent_im_path = os.path.join(config['train_params']['task_name'],\n",
    "                                  config['train_params']['output_train_dir'],\n",
    "                                  'latent_epoch_{}.jpeg')\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        visualize_latent_space(config, model, mnist_test_loader, save_fig_path=latent_im_path.format(0))\n",
    "        model.train()\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        mean_loss = train_for_one_epoch(epoch_idx, model, mnist_loader, optimizer, criterion, config)\n",
    "        if config['train_params']['save_latent_plot']:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                print('Generating latent plot on test set')\n",
    "                visualize_latent_space(config, model, mnist_test_loader,\n",
    "                                       save_fig_path=latent_im_path.format(epoch_idx + 1))\n",
    "            model.train()\n",
    "        scheduler.step(mean_loss)\n",
    "        # Simply update checkpoint if found better version\n",
    "        if mean_loss < best_loss:\n",
    "            print('Improved Loss to {:.4f} .... Saving Model'.format(mean_loss))\n",
    "            torch.save(model.state_dict(), os.path.join(config['train_params']['task_name'],\n",
    "                                                        config['train_params']['ckpt_name']))\n",
    "            best_loss = mean_loss\n",
    "        else:\n",
    "            print('No Loss Improvement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Arguments for conditional vae training')\n",
    "parser.add_argument('--config', dest='config_path',\n",
    "                        default='config/vae_kl.yaml', type=str)\n",
    "args = parser.parse_args()\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(args):\n",
    "    with open(args.config_path, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    print(config)\n",
    "    \n",
    "    model = get_model(config).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(config['train_params']['task_name'],\n",
    "                                                  config['train_params']['ckpt_name']), map_location='cpu'))\n",
    "    model.eval()\n",
    "    mnist = MnistDataset('test', 'data/test/images')\n",
    "    mnist_loader = DataLoader(mnist, batch_size=config['train_params']['batch_size'], shuffle=True, num_workers=4)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        latent_im_path = os.path.join(config['train_params']['task_name'],\n",
    "                                      config['train_params']['output_train_dir'],\n",
    "                                      'latent_inference.jpeg')\n",
    "        visualize_latent_space(config, model, mnist_loader, latent_im_path)\n",
    "        visualize_interpolation(config, model, mnist)\n",
    "        reconstruct(config, model, mnist)\n",
    "        visualize_manifold(config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Arguments for vae inference')\n",
    "parser.add_argument('--config', dest='config_path',\n",
    "                        default='config/vae_kl.yaml', type=str)\n",
    "args = parser.parse_args()\n",
    "inference(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
