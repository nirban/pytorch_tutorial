{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import timeit\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper Parameters\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 40\n",
    "\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 10\n",
    "PATCH_SIZE = 4\n",
    "IMG_SIZE = 28\n",
    "IN_CHANNELS = 1\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.001\n",
    "HIDDEN_DIM = 768\n",
    "ADAM_WEIGHT_DECAY = 0\n",
    "ADAM_BETAS = (0.9, 0.999)\n",
    "ACTVATION = 'gelu'\n",
    "NUM_ENCODER = 4\n",
    "EMBED_DIM = (PATCH_SIZE ** 2) * IN_CHANNELS\n",
    "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(self, embed_dim, patch_size, num_patches, dropout, in_channels):\n",
    "        super(PatchEmbeddings, self).__init__()\n",
    "        self.patcher = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, \n",
    "                out_channels=embed_dim, \n",
    "                kernel_size=patch_size, \n",
    "                stride=patch_size\n",
    "            ),\n",
    "            nn.Flatten(2),\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(size=(1, in_channels, embed_dim)), requires_grad=True)\n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(size=(1, num_patches + 1, embed_dim)), requires_grad=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = x[:, 0, :]\n",
    "        x = self.patcher(x).permute(0, 2, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.positional_embeddings\n",
    "        x = self.dropout(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PatchEmbeddings(embed_dim=EMBED_DIM, patch_size=PATCH_SIZE, num_patches=NUM_PATCHES, dropout=DROPOUT, in_channels=IN_CHANNELS).to(device)\n",
    "\n",
    "x = torch.randn(512, 1 , 28, 28)\n",
    "\n",
    "print(model(x).shape) # torch.Size([512, 50, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_patches, \n",
    "                 img_size,\n",
    "                 num_classes, \n",
    "                 patch_size, \n",
    "                 embed_dim, \n",
    "                 num_encoders, \n",
    "                 num_heads, \n",
    "                 hiden_dim, \n",
    "                 dropout,\n",
    "                 activation, \n",
    "                 in_channels\n",
    "            ):\n",
    "        super(ViT, self).__init__()\n",
    "        self.embeddings_block = PatchEmbeddings(embed_dim, patch_size, num_patches, dropout, in_channels)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dropout=dropout,\n",
    "            activation=activation, \n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "\n",
    "        self.encoder_blocks = nn.TransformerEncoder(encoder_layer, num_layers=num_encoders)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embed_dim),\n",
    "            nn.Linear(in_features=embed_dim, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings_block(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x = self.mlp_head(x[:, 0, :])\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(NUM_PATCHES,IMG_SIZE, NUM_CLASSES, PATCH_SIZE, EMBED_DIM, NUM_ENCODER, NUM_HEADS, HIDDEN_DIM, DROPOUT, ACTVATION, IN_CHANNELS).to(device)\n",
    "x = torch.randn(512, 1 , 28, 28)\n",
    "print(model(x).shape) # torch.Size([512, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/digit_recognizer/train.csv')\n",
    "test_df = pd.read_csv('data/digit_recognizer/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=RANDOM_SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTTrainDataset(Dataset):\n",
    "    def __init__(self, images, labels, indicies, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.indicies = indicies\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            #transforms.RandomHorizontalFlip(),\n",
    "            #transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].reshape(28, 28).astype(np.uint8)\n",
    "        image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        index = self.indicies[idx]\n",
    "\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'index': index\n",
    "        }\n",
    "        return sample\n",
    "    \n",
    "class MNISTValDataset(Dataset):\n",
    "    def __init__(self, images, labels, indicies, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.indicies = indicies\n",
    "        self.transform = transforms.Compose([\n",
    "            #transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].reshape(28, 28).astype(np.uint8)\n",
    "        image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        index = self.indicies[idx]\n",
    "\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'index': index\n",
    "        }\n",
    "        return sample\n",
    "    \n",
    "class MNISTTestDataset(Dataset):\n",
    "    def __init__(self, images, indicies, transform=None):\n",
    "        self.images = images\n",
    "        self.indicies = indicies\n",
    "        self.transform = transforms.Compose([\n",
    "            #transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].reshape(28, 28).astype(np.uint8)\n",
    "        image = self.transform(image)\n",
    "        index = self.indicies[idx]\n",
    "\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'index': index\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "f, axarr = plt.subplots(1, 3)\n",
    "\n",
    "train_dataset = MNISTTrainDataset(train_df.iloc[:, 1:].values.astype(np.uint8), train_df.iloc[:, 0].valus, train_df.index.values)\n",
    "print(len(train_dataset))\n",
    "sample = train_dataset[0]\n",
    "axarr[0].imshow(train_dataset[0]['image'].squeeze(), cmap='gray')\n",
    "axarr[0].set_title(\"Train Image\")\n",
    "print(\"----------------\")\n",
    "\n",
    "val_dataset = MNISTValDataset(val_df.iloc[:, 1:].values.astype(np.uint8), val_df.iloc[:, 0].valus, val_df.index.values)\n",
    "print(len(val_dataset))\n",
    "sample = val_dataset[0]\n",
    "axarr[0].imshow(val_dataset[0]['image'].squeeze(), cmap='gray')\n",
    "axarr[0].set_title(\"Validation Image\")\n",
    "print(\"----------------\")\n",
    "\n",
    "test_dataset = MNISTTestDataset(test_df.iloc[:, :].values.astype(np.uint8), test_df.index.values)\n",
    "print(len(test_dataset))\n",
    "sample = test_dataset[0]\n",
    "axarr[0].imshow(test_dataset[0]['image'].squeeze(), cmap='gray')\n",
    "axarr[0].set_title(\"Test Image\")\n",
    "print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "    def load_checkpoint(self, model):\n",
    "        '''Loads the saved model.'''\n",
    "        model.load_state_dict(torch.load(self.path))\n",
    "\n",
    "\n",
    "# # Initialize the early stopping object\n",
    "# early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "# for epoch in range(1, n_epochs + 1):\n",
    "#     # Train your model\n",
    "#     train(...)\n",
    "    \n",
    "#     # Validate your model\n",
    "#     val_loss = validate(...)\n",
    "    \n",
    "#     # Check early stopping\n",
    "#     early_stopping(val_loss, model)\n",
    "    \n",
    "#     if early_stopping.early_stop:\n",
    "#         print(\"Early stopping\")\n",
    "#         break\n",
    "\n",
    "# # Load the last checkpoint with the best model\n",
    "# early_stopping.load_checkpoint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=ADAM_WEIGHT_DECAY, betas=ADAM_BETAS)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), position=0, leave=True):\n",
    "    model.train()\n",
    "    train_labels = []\n",
    "    train_preds = []\n",
    "    train_running_loss = 0.0\n",
    "\n",
    "    for idx, data_instance in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n",
    "        img = data_instance['image'].float().to(device)\n",
    "        label = data_instance['label'].to(device)\n",
    "        y_pred = model(img)\n",
    "        y_pred_label = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "        train_labels.extend(label.cpu().detach())\n",
    "        train_preds.extend(y_pred_label.cpu().detach())\n",
    "\n",
    "        loss = criterion(y_pred, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_running_loss += loss.item()\n",
    "\n",
    "    train_loss = train_running_loss / (idx + 1)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, data_instance in enumerate(tqdm(val_dataloader, position=0, leave=True)):\n",
    "            img = data_instance['image'].float().to(device)\n",
    "            label = data_instance['label'].to(device)\n",
    "            y_pred = model(img)\n",
    "            y_pred_label = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "            val_labels.extend(label.cpu().detach())\n",
    "            val_preds.extend(y_pred_label.cpu().detach())\n",
    "\n",
    "            loss = criterion(y_pred, label)\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "    val_loss = val_running_loss / (idx + 1)\n",
    "\n",
    "    #accuracy = (np.array(val_labels) == np.array(val_preds)).mean()\n",
    "    #cm = confusion_matrix(val_labels, val_preds)    \n",
    "\n",
    "    train_accuracy = sum(1 for x, y in zip(train_labels, train_preds) if x == y) / len(train_labels)\n",
    "    val_accuracy = sum(1 for x, y in zip(val_labels, val_preds) if x == y) / len(val_labels)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(f'Training_Time: {stop - start}s')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "ids = []\n",
    "imgs = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, data_instance in enumerate(tqdm(test_dataloader, position=0, leave=True)):\n",
    "        img = data_instance['image'].float().to(device)\n",
    "        ids.extend([int(x)+1 for x in data_instance['index']])\n",
    "\n",
    "        y_pred = model(img) # output shape: (batch_size, num_classes)\n",
    "\n",
    "        imgs.extend(img.detach().cpu())\n",
    "        labels.extend([int(i)+1 for i in torch.argmax(y_pred, dim=1).detach().cpu()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "f, axarr = plt.subplots(2, 3)\n",
    "\n",
    "counter = 0\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        axarr[i, j].imshow(imgs[counter].squeeze(), cmap='gray')\n",
    "        axarr[i, j].set_title(f\"Predicted Label: {labels[counter]}\")\n",
    "        counter += 1\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
